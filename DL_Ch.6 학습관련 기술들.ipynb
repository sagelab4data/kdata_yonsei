{"cells":[{"cell_type":"markdown","source":["# 6 . 학습 관련 기술들"],"metadata":{"id":"lWRzef3SaLtW"},"id":"lWRzef3SaLtW"},{"cell_type":"markdown","source":["### 수업 환경 구성을 위한 코드 (수업전 실행)"],"metadata":{"id":"78zpFIX4-iAF"},"id":"78zpFIX4-iAF"},{"cell_type":"markdown","source":["#### MNIST 데이터 적재\n","* common 모듈 사용을 위한 경로 지정시 pickle 오류 발생 문제 회피\n","* 사전에 load_mnist 라이브러리 import 및 MNIST 파일 데이터 load"],"metadata":{"id":"H9Vq6jHQLfbS"},"id":"H9Vq6jHQLfbS"},{"cell_type":"code","source":["# 6.0.1 mnist.py 업로드\n","from google.colab import files\n","src = list(files.upload().values())[0]"],"metadata":{"id":"HGcHUEOkDvNn"},"id":"HGcHUEOkDvNn","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 6.0.2 MNIST 데이터 적재\n","from mnist import load_mnist\n","(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)"],"metadata":{"id":"Qq09BazxD6qP"},"id":"Qq09BazxD6qP","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### common 라이브러리 \n","* 수업 시간에 작성한 여러 함수 및 원활한 프로그램 수행을 위한 utility 등 저자 제공 파일들\n","* common 라이브러리 내 모듈 사용을 위해 google drive mount 및 경로 지정\n","* 사전에 G 드라이브 내 Colab Notebooks 디렉토리에 common 폴더 복제"],"metadata":{"id":"0tDLBlQ4MQFl"},"id":"0tDLBlQ4MQFl"},{"cell_type":"code","source":["# 6.0.3 common 폴더 마운트\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"Ga9tJh_65_GG"},"id":"Ga9tJh_65_GG","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 6.0.4 수행 경로 변경 \n","%cd /content/drive/MyDrive/Colab Notebooks"],"metadata":{"id":"GyGQwF7W7ajJ"},"id":"GyGQwF7W7ajJ","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"demographic-fetish","metadata":{"id":"demographic-fetish"},"source":["## 6.1 매개변수 갱신\n"]},{"cell_type":"markdown","source":["### 6.1.1 모험가 이야기"],"metadata":{"id":"erqb8hULUi82"},"id":"erqb8hULUi82"},{"cell_type":"markdown","source":["### 6.1.2 확률적 경사하강법 (SGD)"],"metadata":{"id":"97dbkcjGBRix"},"id":"97dbkcjGBRix"},{"cell_type":"markdown","id":"attempted-triple","metadata":{"id":"attempted-triple"},"source":["$$ \\mathbf{W} \\gets \\mathbf{W} + \\eta \\frac {\\partial L}{\\partial \\mathbf{W}}$$ "]},{"cell_type":"code","execution_count":null,"id":"circular-applicant","metadata":{"id":"circular-applicant"},"outputs":[],"source":["# 6.1.2.1 확률적 경사하강법 동작 원리\n","class SGD :\n","    def __init__(self, lr = 0.01):\n","        self.lr = lr\n","\n","    def update(self, params, grads):\n","        for key in params.keys():\n","            params[key] -= self.lr * grads[key]"]},{"cell_type":"code","execution_count":null,"id":"right-psychology","metadata":{"id":"right-psychology"},"outputs":[],"source":["\n","# 6.1.2.2 SGD 클래스 적용 예시  psuedo code\n","\"\"\"\n","network = TwoLayerNet\n","optimizer = SGD()\n","\n","for i in range(10000):\n","    ...\n","    x_batch, t_batch = get_minio_batch()\n","    grads = network.gradient (x_batch, t_batch)\n","    params = network.params\n","    optimizer.update(params, grads)\n","\n","\"\"\"\"\"\""]},{"cell_type":"markdown","id":"psychological-friendly","metadata":{"id":"psychological-friendly"},"source":["### 6.1.3 SGD의 단점\n"]},{"cell_type":"markdown","source":["### 6.1.4 모멘텀"],"metadata":{"id":"Esso2siEUPtg"},"id":"Esso2siEUPtg"},{"cell_type":"markdown","id":"manufactured-rhythm","metadata":{"id":"manufactured-rhythm"},"source":["$$ \\mathbf{ v} \\gets   \\alpha \\mathbf{ v}- \\eta \\frac {\\partial L} {\\partial \\mathbf{W}}$$\n","$$ \\mathbf{W} \\gets \\mathbf{W} + \\mathbf{v}$$ "]},{"cell_type":"code","execution_count":null,"id":"opening-rachel","metadata":{"id":"opening-rachel"},"outputs":[],"source":["# 6.1.4.1 모멘텀 동작 원리\n","class Momentum :\n","    def __init__(self, lr= 0.01, momentum = 0.9):\n","        self.lr = lr\n","        self.momentum = momentum\n","        self.v = None\n","        \n","    def update (self, params, grads):\n","        if self.v is None :\n","            self.v = {}\n","            for key, val in params.items():\n","                self.v[key] = np.zeros_like(val)\n","                \n","        for key in params.keys():\n","            self.v[key] = self.momentum * self.v[key] -\\\n","                                self.lr * grads[key]\n","            params[key] += self.v[key]\n","                "]},{"cell_type":"markdown","id":"rational-romance","metadata":{"id":"rational-romance"},"source":["### 6.1.5 AdaGrad"]},{"cell_type":"markdown","id":"enormous-nelson","metadata":{"id":"enormous-nelson"},"source":["$$ \\mathbf{h} \\gets \\mathbf{h}  + \\frac {\\partial L} {\\partial \\mathbf{W}} \\odot \\frac {\\partial L} {\\partial \\mathbf{W}}$$\n","$$ \\mathbf{W} \\gets \\mathbf{W} + \\eta \\frac{1}{ \\sqrt { \\mathbf{h}}} \\frac {\\partial L}{\\partial \\mathbf{W}} $$ "]},{"cell_type":"code","execution_count":null,"id":"sharp-permit","metadata":{"id":"sharp-permit"},"outputs":[],"source":["# 6.1.5.1 AdaGrad 동작 원리\n","class AdaGrad :\n","    def __init__(self, lr = 0.01) :\n","        self.lr = lr\n","        self.h = None\n","        \n","    def update(self, params, grads ):\n","        if self.h == None :\n","            self.h = {}\n","            for key, val in params.items() :\n","                self.h[key] = np.zeros_like(val)\n","        \n","        for key in params.keys():\n","            self.h[key] += grads[key] * grads[key]\n","            params[key] += self.lr *grads[key] / \\\n","                      (np.sqrt(self.h[key]) + 1e-7 )"]},{"cell_type":"markdown","id":"naughty-works","metadata":{"id":"naughty-works"},"source":["### 6.1.6 Adam\n"]},{"cell_type":"markdown","source":["### 6.1.7 어느 갱신 방법을 이용할 것인가"],"metadata":{"id":"GtHLhuVgG0c5"},"id":"GtHLhuVgG0c5"},{"cell_type":"markdown","source":["#### optimizer 별 최소점 접근 경로\n","\n","* 초기값 (-7, 3) 에서 시작하여 각 optimizer의 방법으로 30번 매개변수를 갱신하여 최소점을 찾는 경로 시각화\n","$$ f(x,y) =\\frac {1} {20} x^2 + y^2$$\n","$$ \\frac {\\partial f} {\\partial x} = \\frac {x}{10}$$ \n","$$\\frac {\\partial f} {\\partial y} = 2y $$"],"metadata":{"id":"53DdihWGTmaP"},"id":"53DdihWGTmaP"},{"cell_type":"code","source":["# 6.1.7.1 Optimizer별 최소점 접근 경로 시각화 \n","import numpy as np\n","import matplotlib.pyplot as plt\n","from collections import OrderedDict\n","# 사전 정의된 옵티마이저\n","from common.optimizer import *\n","\n","# 대상이 되는 함수 정의 \n","def f(x, y):\n","    return x**2 / 20.0 + y**2\n","\n","def df(x, y):\n","    return x / 10.0, 2.0*y\n","\n","# 초기값 설정 \n","init_pos = (-7.0, 2.0)\n","\n","# 매개변수와 기울기 초기값\n","params = {}\n","params['x'], params['y'] = init_pos[0], init_pos[1]\n","grads = {}\n","grads['x'], grads['y'] = 0, 0\n","\n","# 옵티마이저 생성 \n","optimizers = OrderedDict()\n","optimizers[\"SGD\"] = SGD(lr=0.95)\n","optimizers[\"Momentum\"] = Momentum(lr=0.1)\n","optimizers[\"AdaGrad\"] = AdaGrad(lr=1.5)\n","optimizers[\"Adam\"] = Adam(lr=0.3)\n","\n","\n","# 4개의 그래프 작성을 위한 figure\n","plt.figure(figsize=(10,10))\n","\n","# 옵티마이저별 탐색\n","for idx, key in enumerate(optimizers, 1 ) :\n","    optimizer = optimizers[key] # 옵티마이저 지정\n","    x_history = [] \n","    y_history = []    # 이동 경로\n","    params['x'], params['y'] = init_pos[0], init_pos[1]  # 초기값 설정\n","    \n","    # optimizer 별로 30 번의 매개변수 갱신을 시도함.\n","    for i in range(30):\n","        x_history.append(params['x'])\n","        y_history.append(params['y'])\n","        \n","        grads['x'], grads['y'] = df(params['x'], params['y']) # 미분값은 구해줌\n","        optimizer.update(params, grads)  ### 매개 변수 갱신 ###\n","    \n","\n","    # 갱신 결과 plotting\n","    # 데이터 생성\n","    x = np.arange(-10, 10, 0.01)\n","    y = np.arange(-5, 5, 0.01)\n","    \n","    X, Y = np.meshgrid(x, y) \n","    Z = f(X, Y)\n","    \n","    # for simple contour line  \n","    mask = Z > 7\n","    Z[mask] = 0\n","    \n","    # plot \n","    plt.subplot(2, 2, idx ) # subplot을 생성하여 그래프 작성\n","\n","    plt.plot(x_history, y_history, 'o-', color=\"red\") # 매개변수 출력\n","    plt.contour(X, Y, Z)\n","    plt.ylim(-10, 10)\n","    plt.xlim(-10, 10)\n","    plt.plot(0, 0, '+')\n","    plt.title(key)\n","    plt.xlabel(\"x\")\n","    plt.ylabel(\"y\")"],"metadata":{"id":"KRA4HzNg5bPh"},"id":"KRA4HzNg5bPh","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"indonesian-billion","metadata":{"id":"indonesian-billion"},"source":["### 6.1.8 MNIST 데이터 셋으로 본 갱신 방법 비교"]},{"cell_type":"markdown","source":["#### optimizer 별 손실 비교 on MNIST"],"metadata":{"id":"qbv8jXeFT1QX"},"id":"qbv8jXeFT1QX"},{"cell_type":"code","execution_count":null,"id":"antique-therapist","metadata":{"scrolled":false,"id":"antique-therapist"},"outputs":[],"source":["# 6.1.8.1 MNIST 분류 손실 비교\n","# 은닉층 4개층 100개 노드 \n","# 은닉층 활성화 함수 : ReLU\n","# 가중치 초기 설정 : He\n","\n","# 배치 사이즈 : 118\n","# 최대 반복 회수 :  2000 iteration\n","\n","# 실험 최적화 optimizer 4종  : SGD, Momentum, AdaGrad, Adam\n","\n","import matplotlib.pyplot as plt\n","\n","#from dataset.mnist import load_mnist\n","from common.util import smooth_curve\n","from common.multi_layer_net import MultiLayerNet\n","from common.optimizer import *\n","\n","# 0:MNIST 데이터 읽어들이기==========\n","(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n","\n","train_size = x_train.shape[0]\n","batch_size = 128\n","max_iterations = 2000\n","\n","# 1:실험 설정==========\n","optimizers = {}\n","optimizers['SGD'] = SGD()\n","optimizers['Momentum'] = Momentum()\n","optimizers['AdaGrad'] = AdaGrad()\n","optimizers['Adam'] = Adam()\n","#optimizers['RMSprop'] = RMSprop()\n","\n","networks = {}\n","train_loss = {}\n","\n","# optimize 별 신경망 구성\n","for key in optimizers.keys():\n","    networks[key] = MultiLayerNet(\n","        input_size=784, \n","        hidden_size_list=[100, 100, 100, 100],\n","        output_size=10)\n","    train_loss[key] = []    \n","\n","\n","# 2: 학습 ==========\n","for i in range(max_iterations):\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_mask]\n","    t_batch = t_train[batch_mask]\n","    \n","    # optimize 별 학습 수행\n","    for key in optimizers.keys():\n","        grads = networks[key].gradient(x_batch, t_batch)\n","        optimizers[key].update(networks[key].params, grads)\n","    \n","        loss = networks[key].loss(x_batch, t_batch)\n","        train_loss[key].append(loss)\n","    \n","    if i % 100 == 0:\n","        print( f\"=========== iteration: {i} ===========\")\n","        for key in optimizers.keys():\n","            loss = networks[key].loss(x_batch, t_batch)\n","            print(f\"{key} : {loss}\")\n"]},{"cell_type":"code","source":["# 6.1.8.2  갱신 방법 별 손실율 그래프 \n","#  3.그래프 작성 =========\n","markers = {\"SGD\": \"o\", \"Momentum\": \"x\", \"AdaGrad\": \"s\", \"Adam\": \"D\"}\n","x = np.arange(max_iterations)\n","\n","plt.figure (figsize = (6,5))\n","\n","for key in optimizers.keys():\n","    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], \n","             markevery=100, label=key)\n","plt.title ('MNIST Loss by optimizer')\n","plt.xlabel(\"iterations\")\n","plt.ylabel(\"loss\")\n","plt.ylim(0, 1)\n","plt.xlim(0,2000)\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"G7aMsOpRLDq8"},"id":"G7aMsOpRLDq8","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"placed-original","metadata":{"id":"placed-original"},"source":["## 6.2 가중치의 초기값\n"]},{"cell_type":"markdown","source":["### 6.2.1 초기값을 0으로 하면"],"metadata":{"id":"ZJSfhdXMUrdH"},"id":"ZJSfhdXMUrdH"},{"cell_type":"markdown","id":"silver-england","metadata":{"id":"silver-england"},"source":["### 6.2.2 은닉층의 활성화값 분포 (sigmoid)"]},{"cell_type":"markdown","source":["#### 기울기 소실  :   $ N (0, 1)$"],"metadata":{"id":"97sHEIZqPLZo"},"id":"97sHEIZqPLZo"},{"cell_type":"code","execution_count":null,"id":"paperback-mattress","metadata":{"id":"paperback-mattress"},"outputs":[],"source":["# 6.2.2.1  은닉층 활성화값 분포 시각화\n","#  5개 은닉층, 각 층 노드 수 100개\n","#  활성화 함수 : 시그모이드 함수\n","#  입력데이터 개수 : 1000 개\n","#  가중치의 초기값 : 표준 정규 분포 N(0,1)\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","x = np.random.randn(1000,100) #1000 개의 입력 데이터 \n","\n","node_num = 100          # 각 은닉층의 노드(뉴런) 수\n","hidden_layer_size = 5   # 은닉층이 5개\n","activations={}          # 활성화 값 저장소\n","\n","for i in range(hidden_layer_size) :\n","    if i != 0 :\n","        x = activations [i-1]   # 이전 출력값이 입력값\n","        \n","    w = np.random.randn(node_num, node_num)*1   #가중치 초기값 : 표준정규분포 \n","    \n","    z = np.dot(x, w)    # 신호 총합\n","    a = sigmoid(z)      # 활성화 \n","    activations[i] = a  # 활성화 값 저장 "]},{"cell_type":"code","execution_count":null,"id":"relevant-drill","metadata":{"id":"relevant-drill"},"outputs":[],"source":["# 6.2.2.2 각 층의 활성화 값 분포 히스토그램 작성\n","\n","fig = plt.figure(figsize=(15,3))\n","fig.suptitle('stddev = 1 Normal Distribution  on Sigmoid', \n","             fontsize = 15 , y = 1.05)\n","\n","for i, a in activations.items():\n","    plt.subplot(1, len(activations), i+1)\n","    plt.title(str (i+1) + '-layer')\n","    plt.hist(a.flatten(), 30, range=(0,1))\n","    \n","plt.show()"]},{"cell_type":"markdown","source":["* 시그모이드 함수는 0과 1 부근에서 미분이 0에 가까워진다.  기울기가 0에 가까워지면 학습이 이어지지 않느다. \n","* 활성화 값이 양 끝으로 어진다는 것은 신호 총합의 절대값이 크다는 의미\n","* 가중치 값을 줄여주어야 한다. "],"metadata":{"id":"txggQKDCVngm"},"id":"txggQKDCVngm"},{"cell_type":"markdown","source":["#### 표현력의 제약 :  $ N (0, 0.01), \\sigma = 0.01$"],"metadata":{"id":"sU14fxXmOM0Q"},"id":"sU14fxXmOM0Q"},{"cell_type":"code","execution_count":null,"id":"headed-andorra","metadata":{"id":"headed-andorra"},"outputs":[],"source":["# 6.2.2.3 위 # 6.2.2.1 과 동일한 조건에서 가중치 초기값을 표준편차 0.01인 정규분포로 변경하여 진행\n","\n","x = np.random.randn(1000,100) #1000 개의 입력 데이터 \n","\n","node_num = 100          # 각 은닉층의 노드(뉴런) 수\n","hidden_layer_size = 5   # 은닉층이 5개\n","activations={}          # 활성화 값 저장소\n","\n","for i in range(hidden_layer_size) :\n","    if i != 0 :\n","        x = activations [i-1]   # 이전 출력값이 입력값\n","        \n","    w = np.random.randn(node_num, node_num)* 0.01   ### 가중치 초기값 변경 : 표준편차 0.01 ### \n","    \n","    z = np.dot(x, w)    # 신호 총합\n","    a = sigmoid(z)      # 활성화 \n","    activations[i] = a  # 활성화 값 저장 \n","    \n","# 5개 층의 분포 시각화 @ stddev = 0.01  on Sigmoid\n","fig = plt.figure(figsize=(15,3))\n","fig.suptitle('stddev = 0.01 Normal Distribution  on Sigmoid', \n","             fontsize = 15 , y = 1.05)\n","\n","for i, a in activations.items():\n","    plt.subplot(1, len(activations), i+1)\n","    plt.title(str (i+1) + '-layer')\n","    plt.hist(a.flatten(), 30, range=(0,1))\n","    \n","plt.show()"]},{"cell_type":"markdown","source":["* 표준편차를 0.01인 정규 분포에 따르는 가중치 초기값은, 매우 0에 근접한 값 -0.03~0.03\n","* 그 결과 시그모이드 활성화 값은 0.5 부근에 집중되어 있다. \n","* 기울기 소실 문제는 해결 되지만, 유사한 가중치 값을 가지면 뉴런을 많이 만든 것이 의미 없게 된다. \n","* **표현력을  제한**하는 문제가 발생한다. "],"metadata":{"id":"rixGT9JqbsYa"},"id":"rixGT9JqbsYa"},{"cell_type":"markdown","source":["#### Xavier 초기값 : $N (0, \\sigma), \\sigma = \\sqrt{1\\over n}$ "],"metadata":{"id":"zxx21e4BOcFG"},"id":"zxx21e4BOcFG"},{"cell_type":"code","execution_count":null,"id":"regional-burke","metadata":{"id":"regional-burke"},"outputs":[],"source":["# 6.2.2.4 위 # 6.2.2.1 과 동일한 조건에서 가중치 초기값을 Xavier 초기값으로 변경하여 진행\n","\n","x = np.random.randn(1000,100) #1000 개의 입력 데이터 \n","\n","node_num = 100          # 각 은닉층의 노드(뉴런) 수\n","hidden_layer_size = 5   # 은닉층이 5개\n","activations={}          # 활성화 값 저장소\n","\n","for i in range(hidden_layer_size) :\n","    if i != 0 :\n","        x = activations [i-1]   # 이전 출력값이 입력값\n","\n","    ### 가중치 초기값 변경 : Xavier ###     \n","    w = np.random.randn(node_num, node_num)/np.sqrt (node_num)   \n","    \n","    z = np.dot(x, w)    # 신호 총합\n","    a = sigmoid(z)      # 활성화 \n","    activations[i] = a  # 활성화 값 저장 \n","    \n","# 5개 층의 분포 시각화 @ Xavier on Sigmoid   \n","fig = plt.figure(figsize=(15,3))\n","fig.suptitle('Xavier on Sigmoid', fontsize = 15 , y = 1.05)\n","\n","for i, a in activations.items():\n","    plt.subplot(1, len(activations), i+1)\n","    plt.title(str (i+1) + '-layer')\n","    plt.hist(a.flatten(), 30, range=(0,1))\n","    \n","plt.show()"]},{"cell_type":"markdown","source":["* Xavier 초기값  표준편차가 앞 계층 노드의  1/√𝑛인 정규 분포에 따르는 가중치.\n","* 1층에서 적절히 넓게 분포 되고, 이후 층이 깊어져도 일정 이상의 분포를 유지한다. "],"metadata":{"id":"6BxUY5t5cCBb"},"id":"6BxUY5t5cCBb"},{"cell_type":"markdown","id":"absolute-duration","metadata":{"id":"absolute-duration"},"source":["### 6.2.3 ReLU를 사용할 때의 가중치 초기값"]},{"cell_type":"markdown","source":["#### $ N (0, \\sigma), \\sigma = 0.01$ on ReLU"],"metadata":{"id":"fVVYzN8SRiRk"},"id":"fVVYzN8SRiRk"},{"cell_type":"code","execution_count":null,"id":"incoming-engine","metadata":{"id":"incoming-engine"},"outputs":[],"source":["# 6.2.3.1 활성화 함수가 ReLU인 경우로 진행한다.  stddev 0.01 의 정규분포\n","\n","def relu(x):\n","    return np.maximum (0, x)\n","\n","x = np.random.randn(1000,100) #1000 개의 입력 데이터 \n","\n","node_num = 100          # 각 은닉층의 노드(뉴런) 수\n","hidden_layer_size = 5   # 은닉층이 5개\n","activations={}          # 활성화 값 저장소\n","\n","for i in range(hidden_layer_size) :\n","    if i != 0 :\n","        x = activations [i-1]   # 이전 출력값이 입력값\n","\n","    ### 가중치 초기값 변경 : stddev = 0.01 정규분포 ###    \n","    w = np.random.randn(node_num, node_num) * 0.01   \n","    \n","    z = np.dot(x, w)\n","    a= relu(z)      ### a = sigmoid(z)를 치환 ###\n","    activations[i] = a\n","    \n","    \n","# 5개 층의 분포 시각화 @ stddev = 0.01 정규분포 on ReLU       \n","fig = plt.figure(figsize=(15,3))\n","fig.suptitle('stddev = 0.01 Normal Distribution on ReLU', fontsize = 15 , y = 1.05)\n","for i, a in activations.items():\n","    plt.subplot(1, len(activations), i+1)\n","    plt.title(str (i+1) + '-layer')\n","    plt.hist(a.flatten(), 30, range=(0,1))\n","    plt.ylim(0, 7000)\n","    \n","plt.show()"]},{"cell_type":"markdown","source":["* 𝑁(0, 0.01) 은 값이 0 으로 쏠린다. 학습이 이루어지지 않게 된다. 기울기 소실 "],"metadata":{"id":"wSouf5yrcVxT"},"id":"wSouf5yrcVxT"},{"cell_type":"markdown","source":["#### Xavier on ReLU : $N (0, \\sigma), \\sigma = \\sqrt{1\\over n}$ "],"metadata":{"id":"gJKCHxsbRnzO"},"id":"gJKCHxsbRnzO"},{"cell_type":"code","execution_count":null,"id":"agricultural-belize","metadata":{"id":"agricultural-belize"},"outputs":[],"source":["# 6.2.3.2 Xavier on ReLU\n","\n","x = np.random.randn(1000,100) #1000 개의 입력 데이터 \n","\n","node_num = 100          # 각 은닉층의 노드(뉴런) 수\n","hidden_layer_size = 5   # 은닉층이 5개\n","activations={}          # 활성화 값 저장소\n","\n","for i in range(hidden_layer_size) :\n","    if i != 0 :\n","        x = activations [i-1]   # 이전 출력값이 입력값\n","\n","    ### 가중치 초기값 변경 : Xavier ###            \n","    w = np.random.randn(node_num, node_num) /np.sqrt (node_num)   \n","    \n","    z = np.dot(x, w)\n","    a= relu(z)      ### a = sigmoid(z)를 치환 ###\n","    activations[i] = a\n","\n","# 5개 층의 분포 시각화 @ Xavier  on ReLU     \n","fig = plt.figure(figsize=(15,3))\n","fig.suptitle('Xavier on ReLU', fontsize = 15, y = 1.05)\n","for i, a in activations.items():\n","    plt.subplot(1, len(activations), i+1)\n","    plt.title(str (i+1) + '-layer')\n","    plt.hist(a.flatten(), 30, range=(0,1))\n","    plt.ylim(0,7000)\n","plt.show()"]},{"cell_type":"markdown","source":["* Xavier 초기값에서도 층이 깊어지면서 치우침이 커지고 기울기 소실 문제가 발생한다. \n","\n"],"metadata":{"id":"FeywuFNFeHQJ"},"id":"FeywuFNFeHQJ"},{"cell_type":"markdown","source":["#### He on ReLU : $N (0, \\sigma), \\sigma = \\sqrt{2\\over n}$ "],"metadata":{"id":"pgvxQAwDRtf_"},"id":"pgvxQAwDRtf_"},{"cell_type":"code","execution_count":null,"id":"arctic-manual","metadata":{"scrolled":true,"id":"arctic-manual"},"outputs":[],"source":["# 6.2.3.3 He 초기값 on ReLU\n","\n","x = np.random.randn(1000,100) #1000 개의 입력 데이터 \n","\n","node_num = 100          # 각 은닉층의 노드(뉴런) 수\n","hidden_layer_size = 5   # 은닉층이 5개\n","activations={}          # 활성화 값 저장소\n","\n","for i in range(hidden_layer_size) :\n","    if i != 0 :\n","        x = activations [i-1]   # 이전 출력값이 입력값\n","\n","    ### 가중치 초기값 변경 : He ###    \n","    w = np.random.randn(node_num, node_num)* np.sqrt (2/node_num)   ## test He\n","    \n","    z = np.dot(x, w)\n","    a= relu(z)     ### a = sigmoid(z)를 치환 ###\n","    activations[i] = a\n","    \n","# 5개 층의 분포 시각화 @ He on ReLU     \n","fig = plt.figure(figsize=(15,3))\n","fig.suptitle('He on ReLU', fontsize = 15, y = 1.05)\n","\n","for i, a in activations.items():\n","    plt.subplot(1, len(activations), i+1)\n","    plt.title(str (i+1) + '-layer')\n","    plt.hist(a.flatten(), 30, range=(0,1))\n","    plt.ylim(0,7000)\n","\n","\n","plt.show()"]},{"cell_type":"markdown","source":["* He 초기값에서는 모든 층에서 균일하게 분포 되었다. \n","* ReLU 함수는 음수에서 0이 되므로,  나머지를 넓게 사용 하기 위해 Xavier 입력값의 2배격인 He 입력값을 채택\n"],"metadata":{"id":"rFPvQGkLeNHG"},"id":"rFPvQGkLeNHG"},{"cell_type":"markdown","id":"radio-treasure","metadata":{"id":"radio-treasure"},"source":["### 6.2.4 MNIST 데이터셋으로 본 가중치 초기값 비교"]},{"cell_type":"markdown","source":["##### 가중치 초기값별 손실비교 on MNIST\n","* weight_init_types = {'std=0.01': 0.01, 'Xavier': 'sigmoid', 'He': 'relu'}\n","* optimizer = SGD(lr=0.01)"],"metadata":{"id":"fThUOjjwOTR_"},"id":"fThUOjjwOTR_"},{"cell_type":"code","execution_count":null,"id":"functional-newcastle","metadata":{"id":"functional-newcastle"},"outputs":[],"source":["# 6.2.4.1 MNIST 데이터셋을 서로 다른 가중치 초기값으로 학습 결과 비교\n","# 은닉층 4개층 100개 노드 \n","# 은닉층 활성화 함수 : ReLU\n","\n","# 배치 사이즈 : 128\n","# 최대 반복 회수 : 2000 iteration\n","# 훈련방식 : SGD   학습률 0.01\n","\n","# 실험 초기값 3종  : std= 0.01,  Xavier, He\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from mnist import load_mnist\n","from common.util import smooth_curve\n","from common.multi_layer_net import MultiLayerNet\n","from common.optimizer import SGD\n","\n","# 0:MNIST 데이터 읽기==========\n","(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n","\n","train_size = x_train.shape[0]\n","batch_size = 128\n","max_iterations = 2000\n","\n","\n","# 1:실험의 설정==========\n","weight_init_types = {'std=0.01': 0.01, 'Xavier': 'sigmoid', 'He': 'relu'}\n","#weight_init_types = {'std=0.01': 0.01, 'Xavier': 0.1, 'He': 0.14} # 은닉층 기준\n","optimizer = SGD(lr=0.01)\n","\n","networks = {}\n","train_loss = {}\n","# 3개의 신경망을 생성\n","for key, weight_type in weight_init_types.items():\n","    networks[key] = MultiLayerNet(input_size=784, \n","                                  hidden_size_list=[100, 100, 100, 100],\n","                                  output_size=10, \n","                                  weight_init_std=weight_type)\n","    train_loss[key] = []\n","\n","\n","# 2:학습==========\n","for i in range(max_iterations):\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_mask]\n","    t_batch = t_train[batch_mask]\n","    \n","    for key in weight_init_types.keys():\n","        grads = networks[key].gradient(x_batch, t_batch)\n","        optimizer.update(networks[key].params, grads)\n","    \n","        loss = networks[key].loss(x_batch, t_batch)\n","        train_loss[key].append(loss)\n","    \n","    if i % 100 == 0:\n","        print(f\"=========== iteration:{i} ===========\")\n","        for key in weight_init_types.keys():\n","            loss = networks[key].loss(x_batch, t_batch)\n","            print(f'{key} : {loss}')\n","\n","\n"]},{"cell_type":"code","source":["# 6.2.4.4 Xavier, He 초기값\n","# 입력층\n","xavier_mnist = np.sqrt(1/784)\n","he_mnist = np.sqrt (2/ 784)\n","# 은닉층 \n","xavier_hidden = np.sqrt(1/100)\n","he_hidden = np.sqrt (2/ 100)\n","xavier_mnist, he_mnist, xavier_hidden, he_hidden"],"metadata":{"id":"rs2eYN7ZgzYc"},"id":"rs2eYN7ZgzYc","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 6.2.4.3 가중치 초기값에 따른 손실감소 그래프 \n","# 3.그래프 ==========\n","markers = {'std=0.01': 'o', 'Xavier': 's', 'He': 'D'}\n","x = np.arange(max_iterations)\n","\n","fig = plt.figure(figsize=(6,5))\n","\n","\n","for key in weight_init_types.keys():\n","    plt.plot(x, smooth_curve(train_loss[key]), \n","             marker=markers[key], \n","             markevery=100, \n","             label=key)\n","    \n","plt.title ('MNIST Loss by initial value ')\n","plt.xlabel(\"iterations\")\n","plt.ylabel(\"loss\")\n","plt.xlim(0,2000)\n","plt.ylim(0, 2.5)\n","plt.legend()\n","plt.show()"],"metadata":{"id":"qIS6EReuOG3d"},"id":"qIS6EReuOG3d","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* std= 0.01의 경우는 거의 학습이 이루어지지 않으나, Xavier, He에서는 순조롭게 학습이 이루어진다. He가 약간 학습속도가 빠르다. "],"metadata":{"id":"r0kjpZbxkHzR"},"id":"r0kjpZbxkHzR"},{"cell_type":"markdown","id":"ultimate-arbitration","metadata":{"id":"ultimate-arbitration"},"source":["## 6.3 뱃치 정규화\n"]},{"cell_type":"markdown","source":["### 6.3.1 뱃치 정규화 알고리즘"],"metadata":{"id":"PRNHgLZBT1m2"},"id":"PRNHgLZBT1m2"},{"cell_type":"markdown","id":"sonic-access","metadata":{"id":"sonic-access"},"source":["$$ \\mu_B \\gets \\frac {1} {m} \\sum^m_{i=1} x_i$$\n","$$\\sigma^2_B \\gets \\frac {1} {m} \\sum^m_{i=1} (x_i-\\mu_B )^2  $$\n","$$ \\hat x_i \\gets \\frac{x_i-\\mu_b}  {\\sqrt {\\sigma^2_B +  \\varepsilon}} $$\n","$$ y_i \\gets \\gamma \\hat x_i + \\beta $$"]},{"cell_type":"markdown","id":"pleasant-uruguay","metadata":{"id":"pleasant-uruguay"},"source":["### 6.3.2 뱃치 정규화의 효과"]},{"cell_type":"markdown","source":["##### 다양한 가중치 초기값에서의 배치정규화 적용여부 on MNIST\n","* use_batchnorm=True vs. Faslse"],"metadata":{"id":"3kf5E2XMM5jP"},"id":"3kf5E2XMM5jP"},{"cell_type":"code","execution_count":null,"id":"sunrise-scott","metadata":{"scrolled":false,"id":"sunrise-scott"},"outputs":[],"source":["# 6.3.2.1 다양한 가중치 초기값에서 배치 정규화 계층 사용여부 비교 > 훈련 환경 준비\n","\n","# 배치 정규화 계층의 사용 비교 실험을 위해 두 훈련을 진행하는 함수 정의 \n","\n","# 은닉층 5개층 100개 노드 \n","# 은닉층 활성화 함수 : ReLU\n","\n","# 배치 사이즈 : 100\n","# 최대 반복 회수 : 20 epoch\n","# 훈련방식 : SGD   학습률 0.01\n","\n","# 훈련데이터 크기 : 1000\n","# 실험 다양한 가중치 초기 설정에서 배치 정규화 적용여부에 따른 차이 비교\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from mnist import load_mnist\n","from common.multi_layer_net_extend import MultiLayerNetExtend\n","from common.optimizer import SGD, Adam\n","\n","(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n","\n","# 학습 데이터의 삭제\n","x_train = x_train[:1000]\n","t_train = t_train[:1000]\n","\n","max_epochs = 20\n","train_size = x_train.shape[0]   # (1000,784)\n","batch_size = 100\n","learning_rate = 0.01\n"]},{"cell_type":"code","source":["# 6.3.2.2 다양한 가중치 초기값에서 배치 정규화 계층 사용여부 비교 > 훈련 함수 정의 \n","\n","# 배치 정규화 계층의 사용 비교 실험을 위해 두 훈련을 진행하는 함수 정의 \n","# 훈련 함수 \n","def __train(weight_init_std):\n","    # 배치 정규화층이 있는 신경망\n","    bn_network = MultiLayerNetExtend(input_size=784, \n","                          hidden_size_list=[100, 100, 100, 100, 100], \n","                          output_size=10, \n","                          weight_init_std=weight_init_std, \n","                          use_batchnorm=True)\n","    \n","    # 대조를 위해 배치 정규화 층이 없는 신경망 \n","    network = MultiLayerNetExtend(input_size=784, \n","                          hidden_size_list=[100, 100, 100, 100, 100], \n","                          output_size=10,\n","                          weight_init_std=weight_init_std)\n","    # 최적화 방식\n","    optimizer = SGD(lr=learning_rate)\n","    \n","    train_acc_list = []\n","    bn_train_acc_list = []\n","    \n","    iter_per_epoch = max(train_size / batch_size, 1)\n","    epoch_cnt = 0\n","    \n","    for i in range(1_000_000_000):  \n","        batch_mask = np.random.choice(train_size, batch_size)\n","        x_batch = x_train[batch_mask]\n","        t_batch = t_train[batch_mask]\n","    \n","        for _network in (bn_network, network):\n","            grads = _network.gradient(x_batch, t_batch)\n","            optimizer.update(_network.params, grads)\n","    \n","        # epoch 마다 정확도 측정\n","        if i % iter_per_epoch == 0:\n","            train_acc = network.accuracy(x_train, t_train)\n","            bn_train_acc = bn_network.accuracy(x_train, t_train)\n","            train_acc_list.append(train_acc)\n","            bn_train_acc_list.append(bn_train_acc)\n","    \n","            print(f'epoch: {epoch_cnt:2} |BN N/A {train_acc:4} - Applied :{bn_train_acc}')\n","    \n","            epoch_cnt += 1\n","            if epoch_cnt >= max_epochs:   # 20 epoch\n","                break\n","                \n","    return train_acc_list, bn_train_acc_list\n"],"metadata":{"id":"79gNzzh7zyKG"},"id":"79gNzzh7zyKG","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"victorian-horizon","metadata":{"id":"victorian-horizon"},"outputs":[],"source":["# 6.3.2.3 다양한 가중치 초기값에서 배치 정규화 계층 사용여부 비교 > 학습 수행\n","\n","## 1 ~ 0.0001 까지 16개 logscale 가중치 초기값 생성\n","weight_scale_list = np.logspace(0, -4, num=16)  \n","\n","\n","#  가중치 초기 설정별 저장 변수 초기화\n","results_bn = {}\n","results_no_bn = {}\n","\n","# 16개 가중치 초기값 별로 학습 수행\n","for i, w in enumerate(weight_scale_list):\n","    print(f\"======== {i+1} /16 @ weight_init_std = {w:.7f} ==============\")\n","    \n","    # 학습 수행\n","    train_acc_list, bn_train_acc_list = __train(w)  \n","   \n","    # 그래프 작성을 위해 학습 수행 결과 저장\n","    results_bn[w] = bn_train_acc_list\n","    results_no_bn[w] = train_acc_list\n","    "]},{"cell_type":"code","source":["# 6.3.2.4 다양한 가중치 초기값에서 배치 정규화 계층 사용여부 비교 > 그래프 \n","\n","# ===== 그래프 작성 =========\n","x = np.arange(max_epochs)\n","plt.figure(figsize = (10,10))\n","plt.suptitle('Comparison batch normalization on various weight_init ')\n","\n","for i, w in enumerate(weight_scale_list):\n","    bn_train_acc_list = results_bn[w]\n","    train_acc_list = results_no_bn[w]\n","\n","    plt.subplot(4,4,i+1)\n","    plt.title (f'W: {w:.7f}')\n","    if i == 15:\n","        plt.plot(x, bn_train_acc_list, label='Batch Normalization', markevery=2)\n","        plt.plot(x, train_acc_list, linestyle = \"--\", label='Normal(without BatchNorm)', markevery=2)\n","    else:\n","        plt.plot(x, bn_train_acc_list, markevery=2)\n","        plt.plot(x, train_acc_list, linestyle=\"--\", markevery=2)\n","\n","    plt.ylim(0, 1.0)\n","    if i % 4:\n","        plt.yticks([])\n","    else:\n","        plt.ylabel(\"accuracy\")\n","    if i < 12:\n","        plt.xticks([])\n","    else:\n","        plt.xlabel(\"epochs\")\n","    if i==15 : \n","        plt.legend(loc='lower right')\n","    \n","plt.show()"],"metadata":{"id":"XFw3jNpJyFAM"},"id":"XFw3jNpJyFAM","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"casual-player","metadata":{"id":"casual-player"},"source":["## 6.4 바른 학습을 위하여\n"]},{"cell_type":"markdown","source":["### 6.4.1 오버피팅"],"metadata":{"id":"SWBUitE4TwuX"},"id":"SWBUitE4TwuX"},{"cell_type":"markdown","source":["##### weight_decay_lambda = 0"],"metadata":{"id":"gEAoK5hcMaTB"},"id":"gEAoK5hcMaTB"},{"cell_type":"code","execution_count":null,"id":"focal-twist","metadata":{"scrolled":true,"id":"focal-twist"},"outputs":[],"source":["# 6.4.1.1 오버피팅을 재현하기 위한 신경망\n","# 은닉층 6개층 100개 노드 \n","# 은닉층 활성화 함수 : ReLU\n","# 가중치 초기설정 : He\n","\n","# 배치 사이즈 : 100\n","# 최대 반복 회수 : 201 epoch\n","# 훈련방식 : SGD   학습률 0.01\n","\n","\n","# 훈련데이터 크기 : 300\n","\n","# 실험 가중치 규제 사용여부에 따른 비교\n","\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from mnist import load_mnist\n","from common.multi_layer_net import MultiLayerNet\n","from common.optimizer import SGD\n","\n","(x_train, t_train), (x_test, t_test) =\\\n","         load_mnist(normalize=True)\n","\n","# 오버피팅을 재현하기 위해서 학습 데이터를 제거 (300 개 )\n","x_train = x_train[:300]\n","t_train = t_train[:300]\n","\n","# weight decay의 설정 =======================\n","weight_decay_lambda = 0 # weight decay를 설정하지 않는 경우\n","# weight_decay_lambda = 0.1  # wd 설정한 경우\n","# ====================================================\n","\n","# 7층 신경망 정의, 학습방식 SGD\n","network = MultiLayerNet(\n","            input_size=784,\n","            hidden_size_list=[100, 100, 100, 100, 100, 100], \n","            output_size=10,\n","            weight_decay_lambda=weight_decay_lambda)\n","optimizer = SGD(lr=0.01)\n","\n","# 반복\n","max_epochs = 201\n","train_size = x_train.shape[0]  # (300,784)\n","batch_size = 100\n","iter_per_epoch = max(train_size / batch_size, 1)\n","epoch_cnt = 0\n","\n","# 성능지표 저장위치 초기화\n","train_loss_list = []\n","train_acc_list = []\n","test_acc_list = []\n","\n","# 학습 수행\n","for i in range(1_000_000_000):\n","    # 1.미니배치\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_mask]\n","    t_batch = t_train[batch_mask]\n","\n","    # 2. 기울기\n","    grads = network.gradient(x_batch, t_batch)\n","    # 3. 갱신\n","    optimizer.update(network.params, grads)\n","\n","    # 에폭마다 훈련 데이터 시험데이터 정확도 산출\n","    if i % iter_per_epoch == 0:\n","        train_acc = network.accuracy(x_train, t_train)\n","        test_acc = network.accuracy(x_test, t_test)\n","        train_acc_list.append(train_acc)\n","        test_acc_list.append(test_acc)\n","\n","        print(f'epoch: {epoch_cnt}, train acc: {train_acc:.4f}, test acc: {test_acc:.4f}')\n","\n","        epoch_cnt += 1\n","        if epoch_cnt >= max_epochs:\n","            break\n"]},{"cell_type":"code","source":["# 6.4.1.2 오버피팅을 재현하기 위한 신경망에서의 정확도 비교\n","\n","# 3.그래프 작성=========\n","\n","fig = plt.figure(figsize=(6,5))\n","\n","markers = {'train': 'o', 'test': 's'}\n","x = np.arange(max_epochs)\n","plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n","plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n","\n","plt.title ('Train vs.Test OverFit w decay = 0')\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"accuracy\")\n","plt.ylim(0, 1.0)\n","plt.xlim(0,200)\n","plt.legend(loc='lower right')\n","plt.show()"],"metadata":{"id":"1yxMyEJ-KyUN"},"id":"1yxMyEJ-KyUN","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"internal-functionality","metadata":{"id":"internal-functionality"},"source":["### 6.4.2 가중치 감소"]},{"cell_type":"markdown","source":["##### weight_decay_lambda = 0.1"],"metadata":{"id":"yDqssRSKMT2I"},"id":"yDqssRSKMT2I"},{"cell_type":"code","source":["# 6.4.2.2 , # 6.4.2.1과 동일, weight_decay_lambda 만 적용\n","# 은닉층 6개층 100개 노드 \n","# 은닉층 활성화 함수 : ReLU\n","# 가중치 초기설정 : He\n","\n","# 배치 사이즈 : 100\n","# 최대 반복 회수 : 201 epoch\n","# 훈련방식 : SGD   학습률 0.01\n","\n","\n","# 훈련데이터 크기 : 300\n","\n","# 실험 가중치 규제 사용여부에 따른 비교\n","\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from mnist import load_mnist\n","from common.multi_layer_net import MultiLayerNet\n","from common.optimizer import SGD\n","\n","(x_train, t_train), (x_test, t_test) =\\\n","         load_mnist(normalize=True)\n","\n","# 오버피팅을 재현하기 위해서 학습 데이터를 제거 (300 개 )\n","x_train = x_train[:300]\n","t_train = t_train[:300]\n","\n","# weight decay의 설정 =======================\n","# weight_decay_lambda = 0 # weight decay를 설정하지 않는 경우\n","weight_decay_lambda = 0.1  # wd 설정한 경우  ###\n","# ====================================================\n","\n","# 7층 신경망 정의, 학습방식 SGD\n","network = MultiLayerNet(\n","            input_size=784,\n","            hidden_size_list=[100, 100, 100, 100, 100, 100], \n","            output_size=10,\n","            weight_decay_lambda=weight_decay_lambda)\n","optimizer = SGD(lr=0.01)\n","\n","# 반복\n","max_epochs = 201\n","train_size = x_train.shape[0]  # (300,784)\n","batch_size = 100\n","iter_per_epoch = max(train_size / batch_size, 1)\n","epoch_cnt = 0\n","\n","# 성능지표 저장위치 초기화\n","train_loss_list = []\n","train_acc_list = []\n","test_acc_list = []\n","\n","# 학습 수행\n","for i in range(1_000_000_000):\n","    # 1.미니배치\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_mask]\n","    t_batch = t_train[batch_mask]\n","\n","    # 2. 기울기\n","    grads = network.gradient(x_batch, t_batch)\n","    # 3. 갱신\n","    optimizer.update(network.params, grads)\n","\n","    # 에폭마다 훈련 데이터 시험데이터 정확도 산출\n","    if i % iter_per_epoch == 0:\n","        train_acc = network.accuracy(x_train, t_train)\n","        test_acc = network.accuracy(x_test, t_test)\n","        train_acc_list.append(train_acc)\n","        test_acc_list.append(test_acc)\n","\n","        print(f'epoch: {epoch_cnt}, train acc: {train_acc:.4f}, test acc: {test_acc:.4f}')\n","\n","        epoch_cnt += 1\n","        if epoch_cnt >= max_epochs:\n","            break"],"metadata":{"id":"j_6UX8wMMpMH"},"id":"j_6UX8wMMpMH","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"healthy-judgment","metadata":{"id":"healthy-judgment"},"outputs":[],"source":["# 6.4.2.2 오버피팅을 재현하기 위한 신경망에서의 정확도 비교 (가중치 감소 적용)\n","# 3.그래프 작성=========\n","\n","fig = plt.figure(figsize=(6,5))\n","\n","markers = {'train': 'o', 'test': 's'}\n","x = np.arange(max_epochs)\n","plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n","plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n","\n","plt.title ('Train vs.Test OverFit w decay = 0.1')\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"accuracy\")\n","plt.ylim(0, 1.0)\n","plt.xlim(0,200)\n","plt.legend(loc='lower right')\n","plt.show()"]},{"cell_type":"markdown","id":"obvious-bernard","metadata":{"id":"obvious-bernard"},"source":["### 6.4.3 드롭아웃"]},{"cell_type":"code","execution_count":null,"id":"female-nevada","metadata":{"id":"female-nevada"},"outputs":[],"source":["# 6.4.3.1 Dropout 계층 정의 \n","class Dropout :\n","    def __init__(self, dropout_ratio = 0.5) :\n","        self.dropout_ratio = dropout_ratio\n","        self.mask = None\n","        \n","    def forward (self, x, train_flg = True) :\n","        if train_flg :\n","            self.mask = np.random.rand (*x.shape) > self.dropout_ratio\n","            return x * self.mask\n","        else :\n","            return x * (1-self.droupout_ratio)\n","        \n","    def backward (self, dout):\n","        return dout * self.mask"]},{"cell_type":"markdown","source":["##### use_dropout = False"],"metadata":{"id":"AaNTNSUkMB6N"},"id":"AaNTNSUkMB6N"},{"cell_type":"code","execution_count":null,"id":"changed-dictionary","metadata":{"scrolled":true,"id":"changed-dictionary"},"outputs":[],"source":["# 6.4.3.2 Dropout 사용하지 않음 (사용본과 대조용)\n","\n","# 은닉층 6개층 100개 노드 \n","# 은닉층 활성화 함수 : ReLU\n","# 가중치 초기설정 : He\n","\n","# 배치 사이즈 : 100\n","# 최대 반복 회수 : 301 epoch\n","# 훈련방식 : SGD   학습률 0.01\n","\n","\n","# 훈련데이터 크기 : 300\n","\n","# 실험 드롭아웃 사용여부에 따른 비교 (dropout_ratio = 0.2)\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from mnist import load_mnist\n","from common.multi_layer_net_extend import MultiLayerNetExtend\n","from common.trainer import Trainer\n","\n","(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n","\n","# 오버피팅을 재현하기 위해서 학습 데이터를 제거\n","x_train = x_train[:300]\n","t_train = t_train[:300]\n","\n","# Dropuout의 유무, 비율의 설정  ========================\n","use_dropout = False  # Dropout을 사용하지 않을 때 False\n","dropout_ratio = 0.2\n","# ====================================================\n","\n","network = MultiLayerNetExtend(\n","            input_size=784, \n","            hidden_size_list=[100, 100, 100, 100, 100, 100],\n","            output_size=10, \n","            use_dropout=use_dropout, \n","            dropout_ration=dropout_ratio)\n","\n","\n","# Trainer 클래스는 간소화 된 훈련 로직 (훈련 로직을 담고 있는 클래스)\n","trainer = Trainer(network, x_train, t_train, x_test, t_test,\n","                  epochs=301, mini_batch_size=100,\n","                  optimizer='sgd', optimizer_param={'lr': 0.01}, \n","                  verbose=True)\n","# 훈련 수행\n","trainer.train()\n","\n","# 훈련 중 정확도 측정 정보\n","train_acc_list, test_acc_list =\\\n","   trainer.train_acc_list, trainer.test_acc_list\n","\n"]},{"cell_type":"code","source":["# 6.4.3.3 Dropout 사용하지 않은 경우\n","# 그래프 생성=========\n","fig = plt.figure(figsize=(6,5))\n","markers = {'train': 'o', 'test': 's'}\n","x = np.arange(len(train_acc_list))\n","plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n","plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"accuracy\")\n","plt.title ('Train vs.Test OverFit w dropout = False')\n","plt.xlim(0, 300)\n","plt.ylim(0, 1.0)\n","plt.legend(loc='lower right')\n","plt.show()"],"metadata":{"id":"YkMoec76KXs2"},"id":"YkMoec76KXs2","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### use_dropout = True"],"metadata":{"id":"1v6RAUTwLtWI"},"id":"1v6RAUTwLtWI"},{"cell_type":"code","execution_count":null,"id":"turkish-interference","metadata":{"scrolled":true,"id":"turkish-interference"},"outputs":[],"source":["# 6.4.3.4 Dropout 사용하지 않음 (# 6.4.3.2 미사용본과 비교)\n","\n","# 은닉층 6개층 100개 노드 \n","# 은닉층 활성화 함수 : ReLU\n","# 가중치 초기설정 : He\n","\n","# 배치 사이즈 : 100\n","# 최대 반복 회수 : 301 epoch\n","# 훈련방식 : SGD   학습률 0.01\n","\n","\n","# 훈련데이터 크기 : 300\n","\n","# 실험 드롭아웃 사용여부에 따른 비교 (dropout_ratio = 0.2)\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from mnist import load_mnist\n","from common.multi_layer_net_extend import MultiLayerNetExtend\n","from common.trainer import Trainer\n","\n","(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n","\n","# 오버피팅을 재현하기 위해서 학습 데이터를 제거\n","x_train = x_train[:300]\n","t_train = t_train[:300]\n","\n","# Dropuout의 유무, 비율의 설정  ========================\n","use_dropout = True  # Dropout을 사용 ***\n","dropout_ratio = 0.2\n","# ====================================================\n","\n","network = MultiLayerNetExtend(\n","            input_size=784, \n","            hidden_size_list=[100, 100, 100, 100, 100, 100],\n","            output_size=10, \n","            use_dropout=use_dropout, \n","            dropout_ration=dropout_ratio)\n","\n","\n","# Trainer 클래스는 간소화 된 훈련 로직 (훈련 로직을 담고 있는 클래스)\n","trainer = Trainer(network, x_train, t_train, x_test, t_test,\n","                  epochs=301, mini_batch_size=100,\n","                  optimizer='sgd', optimizer_param={'lr': 0.01}, \n","                  verbose=True)\n","# 훈련 수행\n","trainer.train()\n","\n","# 훈련 중 정확도 측정 정보\n","train_acc_list, test_acc_list =\\\n","   trainer.train_acc_list, trainer.test_acc_list\n"]},{"cell_type":"code","execution_count":null,"id":"shared-manor","metadata":{"id":"shared-manor"},"outputs":[],"source":["# 6.4.3.5 Dropout 사용한 경우\n","# 그래프 생성=========\n","fig = plt.figure(figsize=(6,5))\n","markers = {'train': 'o', 'test': 's'}\n","x = np.arange(len(train_acc_list))\n","plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n","plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"accuracy\")\n","plt.title ('Train vs.Test OverFit w dropout = True')\n","plt.xlim(0, 300)\n","plt.ylim(0, 1.0)\n","plt.legend(loc='lower right')\n","plt.show()"]},{"cell_type":"markdown","id":"certain-circus","metadata":{"id":"certain-circus"},"source":["## 6.5 적절한 하이퍼파라미터 값 찾기\n"]},{"cell_type":"markdown","source":["### 6.5.1 검증데이터"],"metadata":{"id":"Dbvb8lRxToFS"},"id":"Dbvb8lRxToFS"},{"cell_type":"code","execution_count":null,"id":"supreme-trail","metadata":{"id":"supreme-trail"},"outputs":[],"source":["# 6.5.1.1 훈련데이터에서 검증 데이터 분할 수행을 위해 섞는 (shuffle) 함수\n","\n","def shuffle_dataset(x, t):\n","    permutation = np.random.permutation(x.shape[0])\n","    x = x[permutation,:] if x.ndim == 2 else x[permutation,:,:,:]\n","    t = t[permutation]\n","    return x, t"]},{"cell_type":"code","execution_count":null,"id":"outstanding-adrian","metadata":{"id":"outstanding-adrian"},"outputs":[],"source":["# 6.5.1.2 검증데이터 분할 \n","# MNIST 적재\n","(x_train, t_train) , (x_test, t_test) = load_mnist()\n","\n","# 훈련데이터를 셔플\n","x_train, t_train = shuffle_dataset (x_train, t_train)\n","\n","# 검증데이터의 분할\n","validation_rate = 0.20\n","validation_num = int(x_train.shape[0] * validation_rate)\n","\n","x_val = x_train[:validation_num]\n","t_val = t_train[:validation_num]\n","x_train = x_train[validation_num:]\n","t_train = t_train[validation_num:]"]},{"cell_type":"markdown","id":"fancy-colon","metadata":{"id":"fancy-colon"},"source":["### 6.5.2 하이퍼파라미터의 최적화\n"]},{"cell_type":"markdown","source":["* 0단계 : 하이퍼파리미터 값의 범위를 설정\n","* 1단계 : 설정된 범위에서 하이퍼파라미터의 값을 무작위로 추출\n","* 2단계 : 1단계에서 샘플링한 하이퍼파라미터의 값을 사용하여 학습하고, 검증데이터로 정확도를 평가. (에폭을 작게 설정)\n","* 3단계 : 1단계와 2단계를 특정 횟수 (100 회 등) 반복하며, 그 정확도의 결과를 보고 하이퍼파라미터의 범위를 좁혀나간다. "],"metadata":{"id":"bbM4fCAaf3SX"},"id":"bbM4fCAaf3SX"},{"cell_type":"markdown","source":["### 6.5.3 하이퍼파라미터의 최적화 구현하기 "],"metadata":{"id":"DQ6m7marTlDm"},"id":"DQ6m7marTlDm"},{"cell_type":"markdown","source":["* 하이퍼파라미터 중 가중치 감소 계수 weight_decay와 학습률 learning rate의 적절한 값을 찾아본다. \n"],"metadata":{"id":"lnfA9u1Kehvh"},"id":"lnfA9u1Kehvh"},{"cell_type":"code","execution_count":null,"id":"identified-klein","metadata":{"id":"identified-klein"},"outputs":[],"source":["# 6.5.3.1  가중치감소계수와 학습률 초기 값 무작위 설정 예\n","weight_decay = 10 ** np.random.uniform(-8, -4) # 0.00000001 ~0.0001\n","lr = 10 ** np.random.uniform (-6, -2) #  0.000001 ~ 0.01\n","\n","weight_decay, lr"]},{"cell_type":"markdown","source":["##### Various weight_decay, lr combination : Accuracy on train vs. validation"],"metadata":{"id":"4cxCQEh8NbzK"},"id":"4cxCQEh8NbzK"},{"cell_type":"code","execution_count":null,"id":"otherwise-cache","metadata":{"scrolled":false,"id":"otherwise-cache"},"outputs":[],"source":["# 6.5.3.2 가중치 감소계수와 학습률을 무작위로 100쌍을 조합하여 학습을 진행하고\n","#   검증 정확도가 좋은 20개 조합 (Top 20)의 wd와 lr을 관찰한다.  \n","\n","# 은닉층 6개층 100개 노드 \n","# 은닉층 활성화 함수 : ReLU\n","# 가중치 초기설정 : He\n","\n","# 배치 사이즈 : 100\n","# 최대 반복 회수 : 50 epoch\n","# 훈련방식 : SGD   \n","\n","# 훈련데이터 크기 : 500\n","\n","# 실험 : 하이퍼파라미터 가중치감소계수와 학습률 탐색\n","# weight_decay = 10 ^-8 ~ 10^-4 ( 0.000_000_01 ~ 0.0001)\n","# lr = 10^-6 ~ 10^-2 (0.000_001 ~ 0.01 )\n","\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from mnist import load_mnist\n","from common.multi_layer_net import MultiLayerNet\n","from common.util import shuffle_dataset\n","from common.trainer import Trainer\n","\n","# MNIST 데이터 적재\n","(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n","\n","# 고속화를 위해서 훈련데이터를 제거\n","x_train = x_train[:500]\n","t_train = t_train[:500]\n","\n","# 검증데이터 분리\n","validation_rate = 0.20\n","validation_num = int(x_train.shape[0] * validation_rate)\n","x_train, t_train = shuffle_dataset(x_train, t_train)\n","x_val = x_train[:validation_num]\n","t_val = t_train[:validation_num]\n","x_train = x_train[validation_num:]\n","t_train = t_train[validation_num:]\n","\n","# 훈련 함수 정의\n","def __train(lr, weight_decay, epocs=50):\n","    # 신경망 생성 \n","    network = MultiLayerNet(input_size=784,\n","                            hidden_size_list=[100, 100, 100, 100, 100, 100],\n","                            output_size=10,\n","                            weight_decay_lambda=weight_decay)  #### 탐색대상\n","    \n","    # 트레이너 생성\n","    trainer = Trainer(network,             # 신경망\n","                      x_train, t_train,    # 훈련 데이터 \n","                      x_val, t_val,        # 검증 데이터 \n","                      epochs=epocs,        # 기본 에폭 50\n","                      mini_batch_size=100, # 배치 크기 100\n","                      optimizer='sgd', \n","                      optimizer_param={'lr': lr},  ##### 탐색대상\n","                      verbose=False)\n","    # 훈련 수행\n","    trainer.train()\n","\n","    # 훈련 결과 훈련정확도, 검증정확도 반환\n","    return trainer.test_acc_list, trainer.train_acc_list\n","\n","\n","# 하이퍼파라미터의 랜덤 탐색 ======================================\n","optimization_trial = 100\n","\n","#  하이퍼파라미터 조합 별 학습성능, 검증/훈련데이터별 저장 변수 초기화\n","results_val = {}\n","results_train = {}\n","\n","# 100번 동안 다양한 조합 탐색 \n","for i in range(optimization_trial):\n","\n","    # 탐색할 하이퍼파라미터를 지정된 범위에서 무작위로 설정 ===============\n","\n","    # weight_decay = 10 ^-8 ~ 10^-4 ( 0.000_000_01 ~ 0.0001)\n","    weight_decay = 10 ** np.random.uniform(-8, -4)\n","\n","    # lr = 10^-6 ~ 10^-2 (0.000_001 ~ 0.01 )\n","    lr = 10 ** np.random.uniform(-6, -2)\n","\n","    # =================================================================\n","\n","    # 훈련 수행\n","    val_acc_list, train_acc_list = __train(lr, weight_decay)\n","\n","    # 훈련 수행결과 최종 검증정확도와 lr, weight decay  출력\n","    print(f\"[{i:3}th trial] val acc:{val_acc_list[-1]:6} |\",\n","          f\" weight decay:{weight_decay:.10f}, lr: {lr:.10f}\")\n","        \n","    # 이번 loop에서 수행한 lr값 및 weight_decay 값을 키로 하는 \n","    # results_val, results_train 사전에 각각의 accuracy list 를 저장한다. \n","    key = f\"weight decay:{weight_decay:.10f}, lr:{lr:.10f}\"\n","    results_val[key] = val_acc_list\n","    results_train[key] = train_acc_list"]},{"cell_type":"code","source":["# 6.5.3.3   #6.5.3.2 에서 시도한 100개의 조합 중에서 검증정확도 성능이 가장 좋은 \n","#           20개를 발췌하여 학습과정을 그래프로 표현한다. (훈련 데이터/검증데이터)\n","#           weight_decay = 10 ^-8 ~ 10^-4 ( 0.000_000_01 ~ 0.0001)\n","#           lr = 10^-6 ~ 10^-2 (0.000_001 ~ 0.01 )\n","# 그래프 생성 ========================================================\n","\n","# 그래프 초기화 / 결과 출력 제목\n","plt.figure(figsize = (12,10))\n","print(\"=========== Hyper-Parameter Optimization Result ===========\")\n","\n","# 그래프 그릴 갯수\n","graph_draw_num = 20\n","col_num = 5\n","row_num = int(np.ceil(graph_draw_num / col_num))\n","i = 0\n","\n","# results_val의 값에 저장된 val_acc_list[-1] (=각 리스트 (각 조합의 시도)의 마지막값)의\n","# 내림차순으로 정렬하고 해당 시도의 하이퍼파라미터의 조합을 보여준다. \n","for key, val_acc_list in sorted(results_val.items(), \n","                                key=lambda x:x[1][-1], reverse=True):\n","\n","    print(f\"Best-{i+1:2} (val acc: {val_acc_list[-1]:.2f})| \" + key)\n","\n","    plt.subplot(row_num, col_num, i+1)\n","    plt.title(\"Best-\" + str(i+1))\n","    plt.ylim(0.0, 1.0)\n","    if i % 5: plt.yticks([])\n","    plt.xticks([])\n","    x = np.arange(len(val_acc_list))\n","    plt.plot(x, val_acc_list)\n","    plt.plot(x, results_train[key], \"--\")\n","    i += 1\n","\n","    if i >= graph_draw_num:    # 지정된 상위 개수 출력 후 종료\n","        break\n","\n","plt.show()"],"metadata":{"id":"UqAswXzvKQj2"},"id":"UqAswXzvKQj2","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"DL_Ch.6 학습관련 기술들.ipynb","provenance":[{"file_id":"1gwayG1D4fbt-qc0aHWvQrSmAtrTqvpJM","timestamp":1656714884070}],"collapsed_sections":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}